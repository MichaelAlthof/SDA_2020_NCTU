# -*- coding: utf-8 -*-
"""web_scrape_on_sciencedirect.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CYMWHROdIEm8mu4ghiTSzROg-EFIQRoN
"""

# Web scraping on sciencedirect using scrapy.
# The purpose of this code is scrape an abstract from the article on sciencedirect.
# There are some keywords that used, especially about covid-19.
# The data will analyze with topic modeling methods.
# Author: aziz, muhaimin

# Install scrapy
!pip install scrapy # This step is required if you don't have scrapy library

# Import library
import scrapy
from scrapy.linkextractors import LinkExtractor
import pandas as pd
import os
import json

# Create a scrapy project called Crawling.
# A folder named Crawling will be created in your device.
# After the created, just change the directory into it

!scrapy startproject Crawling # Type this on command prompt.
!cd Crawling # Type this on command prompt to change the directory
os.chdir('/content/Crawling/') # Type this on python.

# Before start to crawl/scrape from the web. Make sure the spider script already in the folder spiders on '/content/Crawling/Crawling/spiders/'.
# The spider script named sciencedirect.py, the file is exist in the github.
# Moreover, overwrite the items.py script on '/content/Crawling/Crawling/' with the items.py script that exist in the github.
# Then strat scraping

!scrapy crawl scidir -O abstract.json # Type this on command prompt to start scraping

# Convert json to dataframe

abstract_dataframe = pd.json_normalize(data)

# This result (data) will be processing first before analyze using LDA/DTM